{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255aafef-661c-4a19-b25f-feda4e1dc461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d5b5f-60b7-46a6-a7a2-476c68d2f8e1",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45370fd8-bec0-4316-90d5-47b3a7470089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    train_url = os.path.join('../data/playground-series-s4e1', \n",
    "                         'train.csv')\n",
    "    test_url = os.path.join('../data/playground-series-s4e1', \n",
    "                            'test.csv')\n",
    "    origin_url = os.path.join('../data/playground-series-s4e1', \n",
    "                              'Churn_Modelling.csv')\n",
    "    if dataset == 'train':\n",
    "        df = pd.read_csv(train_url)\n",
    "    elif dataset == 'test':\n",
    "        df = pd.read_csv(test_url)\n",
    "    elif dataset == 'origin':\n",
    "        df = pd.read_csv(origin_url)\n",
    "    else:\n",
    "        raise ValueError(f'{dataset} is not a supported dataset')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3febde-9e10-4955-8620-fc538c484208",
   "metadata": {},
   "source": [
    "### Target Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af5ff70-ac55-4e2b-8db2-1ce694654def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_target(df, col):\n",
    "    # calcualted the mean exited rate by specified columns\n",
    "    df_target = df.groupby(col).agg({'exited': 'mean'})\n",
    "    df_target = df_target.reset_index()\n",
    "    df_target = df_target.rename(columns={'exited': col+'_target'})\n",
    "    \n",
    "    df = pd.merge(df, df_target, on=col, how='left', \n",
    "                  validate='m:1')\n",
    "    df = df.drop(columns=[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34058f-95af-4135-8100-eb993b8d9bd0",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "Accuracy: roc_auc\n",
    "1. logistic regression\n",
    "2. catboost classifier\n",
    "3. xgboost classifier\n",
    "4. lightgbm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2aeb43cc-a8ea-4b60-9242-738693b6b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df, model='lr', scaler='minmax'):\n",
    "    \n",
    "    X_train = df.drop(columns=['exited'])\n",
    "    y_train = df['exited']\n",
    "    \n",
    "    # stadardization\n",
    "    if scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(f'{scaler} is not supported')\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if model == 'lr':\n",
    "        model = LogisticRegression()\n",
    "    elif model == 'cat':\n",
    "        model = CatBoostClassifier(verbose=0)\n",
    "    elif model == 'xgb':\n",
    "        model = XGBClassifier(verbosity=0)\n",
    "    elif model == 'lgb': \n",
    "        model = LGBMClassifier(verbose=-1)\n",
    "    else:\n",
    "        model = model\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edc9c435-03f7-4aca-a6a4-3c65e3ba7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of lr with minmax is  0.8353.\n",
      "The average score of lr with standard is  0.8353.\n",
      "The average score of cat with minmax is  0.8996.\n",
      "The average score of cat with standard is  0.8995.\n",
      "The average score of xgb with minmax is  0.8981.\n",
      "The average score of xgb with standard is  0.8982.\n",
      "The average score of lgb with minmax is  0.9000.\n",
      "The average score of lgb with standard is  0.8998.\n",
      "CPU times: total: 31min 43s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the data\n",
    "train_df = load_data('train')\n",
    "train_df.columns = train_df.columns.str.lower()\n",
    "train_df = train_df.drop(columns=['id', 'customerid'])\n",
    "\n",
    "# impute the dataset\n",
    "cols = ['surname', 'geography', 'gender', 'hascrcard', 'isactivemember']\n",
    "for col in cols:\n",
    "    train_df = impute_target(train_df, col)\n",
    "    \n",
    "# calculate the roc scores\n",
    "for model in ['lr', 'cat', 'xgb', 'lgb']:\n",
    "    for scaler in ['minmax', 'standard']: \n",
    "        scores = calculate_score(train_df, model=model, scaler=scaler)\n",
    "        print(f'The average score of {model} with {scaler} is {scores.mean(): .4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc48b-e21c-4dcd-910c-026d4adf1332",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10fcadd5-b4be-458e-811f-1c46796f8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from optuna.integration import LightGBMPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2a4f16e-9448-445c-b365-5516e6218878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "        # tree structure\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12, step=1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20), \n",
    "\n",
    "        # better accuracy\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step=0.01),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [4000]),\n",
    "\n",
    "        # combat overfitting\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.99, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 0.99, log=True),\n",
    "        'subsample_freq': trial.suggest_categorical('subsample_freq', [1]), \n",
    "        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_categorical('reg_lambda', [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]), # L2 regularization\n",
    "\n",
    "        'random_state': trial.suggest_categorical('random_state', [42]), \n",
    "        'n_jobs': trial.suggest_categorical('n_jobs', [-1]), \n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = LGBMClassifier(objective='binary', **param_grid)\n",
    "        model.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            eval_set=[(X_test, y_test)], \n",
    "            eval_metric='auc', \n",
    "            early_stopping_rounds=100,\n",
    "            #callbacks=[LightGBMPruningCallback(trial, 'auc'), ], \n",
    "            verbose=0, \n",
    "        )\n",
    "        y_preds = model.predict_proba(X_test)[:, 1]\n",
    "        cv_scores[idx] = roc_auc_score(y_test, y_preds)\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15062134-1809-4d0d-9a15-6e87df106a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-03 17:37:22,272]\u001b[0m A new study created in memory with name: LGBM Classifier\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:38:21,341]\u001b[0m Trial 0 finished with value: 0.8979829346517529 and parameters: {'max_depth': 8, 'num_leaves': 2300, 'learning_rate': 0.02, 'n_estimators': 4000, 'colsample_bytree': 0.406160792434119, 'subsample': 0.8105160683574074, 'subsample_freq': 1, 'reg_alpha': 100.0, 'reg_lambda': 50.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 0 with value: 0.8979829346517529.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:38:26,748]\u001b[0m Trial 1 finished with value: 0.8991263574798459 and parameters: {'max_depth': 3, 'num_leaves': 800, 'learning_rate': 0.27, 'n_estimators': 4000, 'colsample_bytree': 0.6980550343877667, 'subsample': 0.31426771328283737, 'subsample_freq': 1, 'reg_alpha': 0.5, 'reg_lambda': 100.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 1 with value: 0.8991263574798459.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:38:31,405]\u001b[0m Trial 2 finished with value: 0.8997282279039382 and parameters: {'max_depth': 7, 'num_leaves': 600, 'learning_rate': 0.28, 'n_estimators': 4000, 'colsample_bytree': 0.41072753000291357, 'subsample': 0.6034539532685351, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 50.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:39:29,294]\u001b[0m Trial 3 finished with value: 0.8988680768270803 and parameters: {'max_depth': 11, 'num_leaves': 560, 'learning_rate': 0.11, 'n_estimators': 4000, 'colsample_bytree': 0.4837405957901447, 'subsample': 0.5297131528696287, 'subsample_freq': 1, 'reg_alpha': 100.0, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:39:35,160]\u001b[0m Trial 4 finished with value: 0.8910948363633506 and parameters: {'max_depth': 9, 'num_leaves': 1960, 'learning_rate': 0.28, 'n_estimators': 4000, 'colsample_bytree': 0.5186956056092079, 'subsample': 0.21553906068387862, 'subsample_freq': 1, 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:39:44,472]\u001b[0m Trial 5 finished with value: 0.8982077459735063 and parameters: {'max_depth': 11, 'num_leaves': 2340, 'learning_rate': 0.26, 'n_estimators': 4000, 'colsample_bytree': 0.9163624607322092, 'subsample': 0.34204302697158806, 'subsample_freq': 1, 'reg_alpha': 5.0, 'reg_lambda': 50.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:39:49,698]\u001b[0m Trial 6 finished with value: 0.8985746626377921 and parameters: {'max_depth': 12, 'num_leaves': 220, 'learning_rate': 0.19, 'n_estimators': 4000, 'colsample_bytree': 0.46556520078710456, 'subsample': 0.23374766258038815, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:39:57,872]\u001b[0m Trial 7 finished with value: 0.8978266498070759 and parameters: {'max_depth': 11, 'num_leaves': 2860, 'learning_rate': 0.25, 'n_estimators': 4000, 'colsample_bytree': 0.8979251235762217, 'subsample': 0.2356623003373869, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 100.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:09,889]\u001b[0m Trial 8 finished with value: 0.8988797796655517 and parameters: {'max_depth': 5, 'num_leaves': 1620, 'learning_rate': 0.3, 'n_estimators': 4000, 'colsample_bytree': 0.46768928138647115, 'subsample': 0.3276868409438719, 'subsample_freq': 1, 'reg_alpha': 50.0, 'reg_lambda': 50.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 2 with value: 0.8997282279039382.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:16,045]\u001b[0m Trial 9 finished with value: 0.9000693388480119 and parameters: {'max_depth': 3, 'num_leaves': 20, 'learning_rate': 0.23, 'n_estimators': 4000, 'colsample_bytree': 0.6212026322402847, 'subsample': 0.7816056218069728, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:27,248]\u001b[0m Trial 10 finished with value: 0.8975159130712471 and parameters: {'max_depth': 3, 'num_leaves': 1100, 'learning_rate': 0.17, 'n_estimators': 4000, 'colsample_bytree': 0.21980839711188635, 'subsample': 0.9466173619515471, 'subsample_freq': 1, 'reg_alpha': 1.0, 'reg_lambda': 0.5, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:34,261]\u001b[0m Trial 11 finished with value: 0.8987524875287038 and parameters: {'max_depth': 6, 'num_leaves': 20, 'learning_rate': 0.21000000000000002, 'n_estimators': 4000, 'colsample_bytree': 0.30261707481241196, 'subsample': 0.6204850178779594, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 10.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:44,748]\u001b[0m Trial 12 finished with value: 0.8989440962063009 and parameters: {'max_depth': 5, 'num_leaves': 500, 'learning_rate': 0.12, 'n_estimators': 4000, 'colsample_bytree': 0.3352410594947149, 'subsample': 0.6862729780896342, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:48,934]\u001b[0m Trial 13 finished with value: 0.8985698095132273 and parameters: {'max_depth': 7, 'num_leaves': 1200, 'learning_rate': 0.22, 'n_estimators': 4000, 'colsample_bytree': 0.6422422952911847, 'subsample': 0.4605647606912421, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:53,226]\u001b[0m Trial 14 finished with value: 0.9000641564036567 and parameters: {'max_depth': 4, 'num_leaves': 400, 'learning_rate': 0.24000000000000002, 'n_estimators': 4000, 'colsample_bytree': 0.6333431489097099, 'subsample': 0.7185616537194712, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 9 with value: 0.9000693388480119.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:40:59,468]\u001b[0m Trial 15 finished with value: 0.9004909479469244 and parameters: {'max_depth': 4, 'num_leaves': 120, 'learning_rate': 0.15000000000000002, 'n_estimators': 4000, 'colsample_bytree': 0.6741203460358444, 'subsample': 0.9818026600578826, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 15 with value: 0.9004909479469244.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:41:06,147]\u001b[0m Trial 16 finished with value: 0.9002637460015203 and parameters: {'max_depth': 4, 'num_leaves': 20, 'learning_rate': 0.12, 'n_estimators': 4000, 'colsample_bytree': 0.7586556460189151, 'subsample': 0.9095105732634332, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 15 with value: 0.9004909479469244.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:41:12,912]\u001b[0m Trial 17 finished with value: 0.9003931984372547 and parameters: {'max_depth': 5, 'num_leaves': 900, 'learning_rate': 0.08, 'n_estimators': 4000, 'colsample_bytree': 0.8000796240210971, 'subsample': 0.9872210350502375, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 15 with value: 0.9004909479469244.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:41:26,215]\u001b[0m Trial 18 finished with value: 0.9006759013425482 and parameters: {'max_depth': 5, 'num_leaves': 960, 'learning_rate': 0.04, 'n_estimators': 4000, 'colsample_bytree': 0.8040386238255977, 'subsample': 0.9545651481074281, 'subsample_freq': 1, 'reg_alpha': 1.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 18 with value: 0.9006759013425482.\u001b[0m\n",
      "\u001b[32m[I 2024-01-03 17:42:09,333]\u001b[0m Trial 19 finished with value: 0.9005997799361907 and parameters: {'max_depth': 6, 'num_leaves': 1500, 'learning_rate': 0.01, 'n_estimators': 4000, 'colsample_bytree': 0.9539095290872474, 'subsample': 0.4659365528126125, 'subsample_freq': 1, 'reg_alpha': 1.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1}. Best is trial 18 with value: 0.9006759013425482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 12min 26s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = train_df.drop(columns=['exited'])\n",
    "y_train = train_df['exited']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='LGBM Classifier')\n",
    "func = lambda trial: objective(trial, X_train_scaled, y_train)\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6ef6c8c-6836-4714-a74a-e3b5f1a92547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value： 0.90068\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tBest value： {study.best_value:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4a6c8-b305-42f6-b9bb-4930816a78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(objective='binary', **study.best_params)\n",
    "scores = calculate_score(train_df, model)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65a856-ea2c-41ca-94d5-8b4743ce9bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GradientBoostingDecisionTrees",
   "language": "python",
   "name": "gradientboosting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
